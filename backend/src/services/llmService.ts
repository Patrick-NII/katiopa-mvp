import { ChatOpenAI } from '@langchain/openai';
import { PrismaClient } from '@prisma/client';
import { PromptTemplate } from '@langchain/core/prompts';
import { StringOutputParser } from '@langchain/core/output_parsers';
import { RunnableSequence } from '@langchain/core/runnables';
import { Tool } from '@langchain/core/tools';
import ragService from './ragService';

const prisma = new PrismaClient();

// Configuration du mod√®le OpenAI
const llm = new ChatOpenAI({
  modelName: 'gpt-4o-mini', // ou 'gpt-4' pour plus de puissance
  temperature: 0.7,
  openAIApiKey: process.env.OPENAI_API_KEY,
});

// Outils personnalis√©s pour acc√©der aux donn√©es
class DatabaseQueryTool extends Tool {
  name = 'database_query';
  description = 'Interroge la base de donn√©es pour obtenir des informations sur les utilisateurs, sessions, activit√©s, etc.';

  async _call(input: string) {
    try {
      // Analyse de la requ√™te pour d√©terminer ce qui est demand√©
      const query = input.toLowerCase();
      
      if (query.includes('enfant') || query.includes('session') || query.includes('compte')) {
        // R√©cup√©ration des informations sur les sessions utilisateur
        const userSessions = await prisma.userSession.findMany({
          include: {
            user: {
              include: {
                account: true,
                activities: true,
              }
            },
            account: true,
          }
        });
        
        return JSON.stringify(userSessions, null, 2);
      }
      
      if (query.includes('activit√©') || query.includes('exercice') || query.includes('progr√®s')) {
        // R√©cup√©ration des activit√©s et progr√®s
        const activities = await prisma.activity.findMany({
          include: {
            userSession: {
              include: {
                user: true,
                account: true,
              }
            }
          },
          orderBy: {
            createdAt: 'desc'
          }
        });
        
        return JSON.stringify(activities, null, 2);
      }
      
      if (query.includes('statistique') || query.includes('performance')) {
        // R√©cup√©ration des statistiques
        const stats = await prisma.activity.groupBy({
          by: ['domain'],
          _count: {
            id: true
          },
          _avg: {
            score: true,
            durationMs: true
          }
        });
        
        return JSON.stringify(stats, null, 2);
      }
      
      return "Requ√™te non reconnue. Utilisez des mots-cl√©s comme 'enfant', 'session', 'activit√©', 'progr√®s', 'statistique'.";
      
    } catch (error) {
      return `Erreur lors de la requ√™te: ${error.message}`;
    }
  }
}

class ProjectArchitectureTool extends Tool {
  name = 'project_architecture';
  description = 'Fournit des informations sur l\'architecture du projet Katiopa, les composants, et la logique m√©tier.';

  async _call(input: string) {
    const architecture = {
      project: "Katiopa MVP - Plateforme d'apprentissage IA",
      description: "Plateforme √©ducative bas√©e sur l'IA pour l'apprentissage personnalis√© des enfants",
      architecture: {
        frontend: "Next.js avec React, Tailwind CSS, Framer Motion",
        backend: "Node.js avec Express, Prisma ORM, PostgreSQL",
        ai: "OpenAI GPT-4, LangChain pour l'orchestration",
        authentication: "JWT, sessions utilisateur, gestion des r√¥les"
      },
      businessLogic: {
        userTypes: ["CHILD", "PARENT", "TEACHER", "ADMIN"],
        subscriptionTypes: ["FREE", "PRO", "PRO_PLUS", "ENTERPRISE"],
        sessionManagement: "Gestion des sessions enfants/parents avec temps de connexion",
        learningDomains: ["Math√©matiques", "Programmation", "Lecture", "Sciences", "IA & Logique"],
        aiEvaluation: "√âvaluation continue des progr√®s avec recommandations personnalis√©es"
      },
      dataStructure: {
        accounts: "Comptes utilisateur avec abonnements",
        userSessions: "Sessions actives des enfants et parents",
        activities: "Activit√©s d'apprentissage avec scores et dur√©es",
        userProfiles: "Profils d√©taill√©s avec pr√©f√©rences et objectifs"
      }
    };
    
    return JSON.stringify(architecture, null, 2);
  }
}

// Prompt principal pour l'Assistant IA Katiopa
const createMainPrompt = (userType: string, firstName: string, subscriptionType: string) => {
  return PromptTemplate.fromTemplate(`
Tu es l'Assistant IA Katiopa, une √©quipe p√©dagogique virtuelle d'excellence repr√©sentant une √©cole r√©put√©e pour former les meilleurs cerveaux.

CONTEXTE UTILISATEUR:
- Type: ${userType}
- Pr√©nom: ${firstName}
- Abonnement: ${subscriptionType}

TON R√îLE:
Tu es un expert p√©dagogique de niveau international avec:
- Expertise en neurosciences cognitives
- M√©thodes d'apprentissage √©prouv√©es (Montessori, Freinet, etc.)
- Capacit√© d'analyse fine des progr√®s
- Anticipation des besoins √©ducatifs
- √âvaluation continue et personnalis√©e

MISSION:
1. Analyser les donn√©es de l'utilisateur avec pr√©cision
2. Fournir des insights p√©dagogiques de qualit√©
3. Anticiper les besoins d'apprentissage
4. Donner des recommandations personnalis√©es
5. Cr√©er un lien √©motionnel et motivant

STYLE DE COMMUNICATION:
- ${userType === 'PARENT' ? 'Professionnel mais chaleureux, rassurant, orient√© r√©sultats' : 'Encourageant, adapt√© √† l\'√¢ge, ludique et motivant'}
- Utilise le pr√©nom de l'utilisateur
- Sois pr√©cis et concret dans tes analyses
- Donne des exemples pratiques
- Reste toujours positif et constructif

QUESTION DE L'UTILISATEUR: {question}

R√âPONSE:
`);
};

// Service principal LLM
export class LLMService {
  private tools: Tool[];
  
  constructor() {
    this.tools = [
      new DatabaseQueryTool(),
      new ProjectArchitectureTool(),
    ];
  }

  async processUserQuery(
    question: string,
    userType: string,
    firstName: string,
    subscriptionType: string,
    focus?: string,
    userSessionId?: string,
    accountId?: string
  ): Promise<string> {
    try {
      // Utilisation du service RAG pour une r√©ponse enrichie
      console.log('üß† Utilisation du service RAG avanc√©...');
      
      const response = await ragService.generateRAGResponse(
        question,
        userType,
        firstName,
        subscriptionType,
        focus,
        userSessionId,
        accountId
      );

      return response;
      
    } catch (error) {
      console.error('Erreur LLM Service:', error);
      
      // Fallback vers la m√©thode classique si le RAG √©choue
      console.log('üîÑ Fallback vers la m√©thode classique...');
      try {
        const prompt = createMainPrompt(userType, firstName, subscriptionType);
        
        const chain = RunnableSequence.from([
          prompt,
          llm,
          new StringOutputParser(),
        ]);

        const fallbackResponse = await chain.invoke({
          question: `${question}\n\nContexte suppl√©mentaire: ${focus ? `Mati√®re de focus: ${focus}` : ''}`,
        });

        // Sauvegarde de la conversation de fallback
        if (userSessionId && accountId) {
          try {
            await this.saveConversation(
              userSessionId,
              accountId,
              question,
              fallbackResponse,
              focus,
              userType,
              subscriptionType
            );
          } catch (saveError) {
            console.error('Erreur lors de la sauvegarde de la conversation de fallback:', saveError);
          }
        }

        return fallbackResponse;
        
      } catch (fallbackError) {
        console.error('Erreur lors du fallback:', fallbackError);
        return `D√©sol√©, je rencontre une difficult√© technique. Veuillez r√©essayer dans quelques instants. Erreur: ${error.message}`;
      }
    }
  }

  // M√©thode priv√©e pour sauvegarder une conversation
  private async saveConversation(
    userSessionId: string,
    accountId: string,
    message: string,
    response: string,
    focus?: string,
    userType?: string,
    subscriptionType?: string
  ): Promise<void> {
    try {
      const context = {
        userType,
        subscriptionType,
        timestamp: new Date().toISOString(),
        model: 'gpt-4o-mini',
        focus
      };

      const metadata = {
        messageLength: message.length,
        responseLength: response.length,
        estimatedTokens: Math.ceil((message.length + response.length) / 4), // Estimation approximative
        processingTime: new Date().toISOString()
      };

      await prisma.conversation.create({
        data: {
          userSessionId,
          accountId,
          message,
          response,
          focus,
          context,
          metadata
        }
      });

      console.log('‚úÖ Conversation sauvegard√©e avec succ√®s');
    } catch (error) {
      console.error('‚ùå Erreur lors de la sauvegarde de la conversation:', error);
      throw error;
    }
  }

  async getContextualResponse(
    userType: string,
    firstName: string,
    subscriptionType: string
  ): Promise<string> {
    try {
      const contextualPrompt = PromptTemplate.fromTemplate(`
Tu es l'Assistant IA Katiopa. G√©n√®re une r√©ponse contextuelle et personnalis√©e pour:

Utilisateur: ${firstName} (${userType})
Abonnement: ${subscriptionType}

Donne une analyse rapide, des encouragements et des recommandations personnalis√©es.
Sois chaleureux, professionnel et motivant.
`);

      const chain = RunnableSequence.from([
        contextualPrompt,
        llm,
        new StringOutputParser(),
      ]);

      const response = await chain.invoke({});
      return response;
      
    } catch (error) {
      console.error('Erreur r√©ponse contextuelle:', error);
      return `Bonjour ${firstName} ! Je suis ravi de vous accompagner dans votre apprentissage avec Katiopa.`;
    }
  }
}

export default new LLMService();
