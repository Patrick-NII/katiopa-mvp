// app/api/chat/route.ts
// Optional LLM fallback. Requires OPENAI_API_KEY. You can swap provider if needed.
import { NextRequest, NextResponse } from 'next/server'
import { getUserSubscription, getModelForSubscription, isLLMEnabled, getMaxTokensForSubscription } from '@/lib/chatbot/auth'

type ChatMsg = { role: 'system' | 'user' | 'assistant'; content: string }
type ReqBody = { system?: string; messages: Array<{ id:string; text:string; sender:'user'|'bot'; timestamp:number }> }

export async function POST(request: NextRequest) {
  try {
    const body = await request.json() as ReqBody
    
    // R√©cup√©rer les informations d'abonnement de l'utilisateur
    const userSubscription = await getUserSubscription()
    
    if (!userSubscription) {
      return NextResponse.json({ 
        text: "Vous devez √™tre connect√© pour utiliser le chatbot avec LLM.", 
        actions: [],
        error: 'NOT_AUTHENTICATED'
      })
    }

    // V√©rifier si le LLM est activ√© pour cet abonnement
    // D√©bloqu√© pour tous les comptes (sp√©cialement Aylon-007)
    if (!isLLMEnabled(userSubscription.subscriptionType)) {
      return NextResponse.json({ 
        text: "Bubix est maintenant disponible pour tous les abonnements ! üéâ", 
        actions: [],
        error: 'LLM_NOT_AVAILABLE'
      })
    }

    const system = body.system ?? "Tu es Bubix, l'assistant IA de CubeAI. Sois clair, utile, bienveillant et adapt√© aux enfants. Oriente vers des liens internes quand pertinent."
    const history: ChatMsg[] = [{ role: 'system', content: system }]

    for (const m of body.messages.slice(-12)) {
      history.push({ role: m.sender === 'user' ? 'user' : 'assistant', content: m.text })
    }

    const key = process.env.OPENAI_API_KEY
    if (!key || key === 'sk-your-openai-api-key-here') {
      return NextResponse.json({ 
        text: "Le mode LLM n'est pas configur√© c√¥t√© serveur. Contactez l'administrateur pour activer la r√©ponse intelligente.", 
        actions: [],
        error: 'LLM_NOT_CONFIGURED'
      })
    }

    // Utiliser le mod√®le appropri√© selon l'abonnement
    const model = getModelForSubscription(userSubscription.subscriptionType)
    const maxTokens = getMaxTokensForSubscription(userSubscription.subscriptionType)

    // Call OpenAI Chat Completions
    const payload = {
      model: model,
      messages: history,
      temperature: 0.6,
      max_tokens: maxTokens,
    }

    const r = await fetch("https://api.openai.com/v1/chat/completions", {
      method: 'POST',
      headers: {
        'Content-Type': 'application/json',
        'Authorization': `Bearer ${key}`
      },
      body: JSON.stringify(payload),
    })

    if (!r.ok) {
      const txt = await r.text()
      return NextResponse.json({ 
        text: "Le service LLM a renvoy√© une erreur. Fallback local activ√©.", 
        actions: [], 
        info: txt,
        error: 'LLM_ERROR'
      })
    }
    const data = await r.json()
    const text = data.choices?.[0]?.message?.content ?? "R√©ponse vide du mod√®le."
    
    return NextResponse.json({ 
      text, 
      actions: [],
      model: model,
      subscriptionType: userSubscription.subscriptionType
    })
  } catch (e: any) {
    return NextResponse.json({ 
      text: "Impossible d'interroger le LLM pour le moment. Utilisez les liens et le moteur local.", 
      actions: [],
      error: 'LLM_UNAVAILABLE'
    })
  }
}
